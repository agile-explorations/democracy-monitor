# Democracy Monitor â€” Document Search Specification

## Document Purpose

This specification describes search capabilities built on top of the existing document corpus with embeddings (pgvector, 1536-dimensional). It is designed as an addendum to the UI Design Specification V3 and follows the same conventions (reading level toggle, four-layer depth model, ASCII wireframes).

Route: `/search`

Added to the top navigation bar between "P2025" and "Health."

---

## 1. Two Audiences, Two Modes

The search page serves two distinct workflows through a single interface that adapts based on how the user interacts with it.

### 1.1 Research Mode (Default)

**Audience**: Journalists, democracy academics, engaged citizens.

**Core question**: "What has the government actually done about X?"

The user types a natural language question. The system retrieves the most relevant government documents via semantic search, then synthesizes an answer grounded in the documentary record. Every claim in the answer links to the specific document(s) that support it.

**Why this matters**: Journalists can ask "Has the administration taken steps to reduce inspector general independence?" and get an answer citing specific Federal Register documents, executive orders, and policy actions â€” grounded in the actual government record. News coverage from the GDELT media corpus is shown alongside as context, but the synthesized answer is built from government documents only. This separation is the system's core value proposition: what the government _did_ vs. what was _reported about it_.

### 1.2 Explore Mode

**Audience**: Developers, methodology contributors, analysts.

**Core question**: "Show me the raw data â€” what did the system see and how did it score it?"

The user enters keywords, filters by category/date/score/document class, and browses the scored document corpus directly. Results show scoring details: which keywords matched, which were suppressed, the severity score, the document class multiplier, and the AI reviewer's assessment.

**Why this matters**: Contributors can find edge cases ("show me all documents in `civilService` that scored above 8 but the AI downgraded"), verify scoring decisions, and identify systematic false positives.

---

## 2. Page Structure

The page has a single search input at the top. Below it, a mode toggle determines how results are displayed. The mode toggle persists via localStorage.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search the Documentary Record                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ğŸ”  â”‚
â”‚  â”‚  What has the government done about...            â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  Mode: [â— Research | â—‹ Explore]                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  (results area â€” varies by mode)                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Search input behavior**:

- In Research mode: placeholder is "Ask a question about the government record..." Submit triggers semantic search + RAG synthesis.
- In Explore mode: placeholder is "Search documents by keyword, title, or content..." Submit triggers combined keyword + semantic search with filters visible.

---

## 3. Research Mode â€” "Ask the Record"

### 3.1 How It Works (Backend)

1. User submits a natural language question
2. System embeds the question using the same model as document embeddings (text-embedding-3-small or equivalent)
3. pgvector cosine similarity search against `documents.embedding`, returning top 20 candidates **filtered to government sources only** (`source_type NOT IN ('gdelt', 'news')`)
4. Candidates are re-ranked by relevance (cosine similarity Ã— recency boost for time-sensitive queries)
5. Top 8â€“12 government documents are sent to the LLM with the user's question and a grounding prompt
6. LLM generates an answer where every factual claim cites specific documents by title and URL
7. The full list of retrieved government documents is shown below the answer
8. **Separately**, a second vector search runs against news/GDELT documents for the same query, returning top 5â€“8 news articles. These are displayed in a "News Coverage" panel below the government document list â€” they are _not_ sent to the LLM for synthesis

**Grounding prompt** (for the LLM synthesis step):

```
You are answering a question about U.S. government actions based solely
on the documents provided below. These are real government documents from
the Federal Register and other official sources.

Rules:
1. Only make claims supported by the provided documents
2. Cite each claim with [Doc N] where N matches the document number below
3. If the documents don't contain enough information to answer, say so
4. Note the date range of available documents â€” the user should know
   what time period the answer covers
5. If documents suggest conflicting actions, present both
6. Do not editorialize or assess democratic health â€” present what the
   documents show
```

**API endpoint**: `GET /api/search?q={query}&mode=research`

Response includes:

- `answer`: The synthesized text with document citations (government sources only)
- `documents`: Array of retrieved government documents with scores, metadata, and relevance ranking
- `relatedNews`: Array of news/GDELT articles matching the same query (not used in synthesis)
- `dateRange`: { earliest, latest } of retrieved government documents
- `confidence`: How well the retrieved documents match the query (average cosine similarity of top results)

### 3.2 Research Mode Results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search the Documentary Record                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ğŸ”  â”‚
â”‚  â”‚  Has the administration reduced IG independence?  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  Mode: [â— Research | â—‹ Explore]                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  ANSWER  Â·  Based on 9 government documents                 â”‚
â”‚          Â·  Jan 2025 â€“ Feb 2026                             â”‚
â”‚                                                             â”‚
â”‚  Several documents indicate changes to inspector general    â”‚
â”‚  authority and independence. An executive order issued on    â”‚
â”‚  January 24, 2025 [1] removed inspectors general from      â”‚
â”‚  multiple federal agencies simultaneously. A subsequent     â”‚
â”‚  Federal Register notice [3] restructured reporting         â”‚
â”‚  requirements for remaining IGs, directing reports through  â”‚
â”‚  agency heads rather than directly to Congress.             â”‚
â”‚                                                             â”‚
â”‚  No documents were found reversing or moderating these      â”‚
â”‚  changes. A GAO report [7] noted the removals but did not  â”‚
â”‚  assess their legality.                                     â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  GOVERNMENT RECORD (9 results, ranked by relevance)         â”‚
â”‚  These are official government documents â€” the basis for    â”‚
â”‚  the answer above.                                          â”‚
â”‚                                                             â”‚
â”‚  [1] â–ˆâ–ˆ 0.94  Executive Order: Accountability for IGs      â”‚
â”‚      Jan 24, 2025 Â· executiveActions Â· Score: 14.2          â”‚
â”‚      ğŸ“„ Federal Register                                    â”‚
â”‚      "...removal of inspectors general who have failed..."  â”‚
â”‚      â†’ federalregister.gov/documents/2025/01/24/...         â”‚
â”‚                                                             â”‚
â”‚  [2] â–ˆâ–ˆ 0.91  Personnel Actions: Office of Inspector Gen.  â”‚
â”‚      Feb 3, 2025 Â· civilService Â· Score: 11.7               â”‚
â”‚      ğŸ“„ Federal Register                                    â”‚
â”‚      "...positions reclassified under Schedule F..."        â”‚
â”‚      â†’ federalregister.gov/documents/2025/02/03/...         â”‚
â”‚                                                             â”‚
â”‚  [3] â–ˆâ–ˆ 0.87  IG Reporting Requirements Amendment          â”‚
â”‚      Feb 18, 2025 Â· igs Â· Score: 9.3                        â”‚
â”‚      ğŸ“„ Federal Register                                    â”‚
â”‚      "...quarterly reports submitted to agency head..."     â”‚
â”‚      â†’ federalregister.gov/documents/2025/02/18/...         â”‚
â”‚                                                             â”‚
â”‚  ... (6 more)                                               â”‚
â”‚  [Show all 9 documents]                                     â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  NEWS COVERAGE (5 articles)                                 â”‚
â”‚  Media reporting on the same topic â€” not used in the        â”‚
â”‚  answer above. Provides public narrative context.           â”‚
â”‚                                                             â”‚
â”‚  â–ˆâ–ˆ 0.91  "Mass IG Firing Raises Alarm Among Oversight      â”‚
â”‚           Experts"                                          â”‚
â”‚      Jan 25, 2025 Â· ğŸ“° Washington Post                      â”‚
â”‚      "...unprecedented removal of multiple inspectors..."   â”‚
â”‚      â†’ washingtonpost.com/politics/2025/01/25/...           â”‚
â”‚                                                             â”‚
â”‚  â–ˆâ–ˆ 0.88  "What the IG Removals Mean for Government         â”‚
â”‚           Accountability"                                   â”‚
â”‚      Jan 27, 2025 Â· ğŸ“° ProPublica                           â”‚
â”‚      "...the offices that investigate waste, fraud..."      â”‚
â”‚      â†’ propublica.org/article/inspector-general-...         â”‚
â”‚                                                             â”‚
â”‚  â–ˆâ–ˆ 0.83  "Administration Defends IG Restructuring as       â”‚
â”‚           'Long Overdue Reform'"                            â”‚
â”‚      Feb 5, 2025 Â· ğŸ“° Fox News                              â”‚
â”‚      "...officials argue the changes improve efficiency..." â”‚
â”‚      â†’ foxnews.com/politics/ig-reform-...                   â”‚
â”‚                                                             â”‚
â”‚  ... (2 more)                                               â”‚
â”‚  [Show all 5 articles]                                      â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  RELATED QUESTIONS                                          â”‚
â”‚  Â· What oversight mechanisms remain for removed IGs?        â”‚
â”‚  Â· Has Congress responded to IG restructuring?              â”‚
â”‚  Â· How does this compare to prior IG changes?               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Research Mode Design Notes

**Relevance bar**: The `â–ˆâ–ˆ` bar next to each document number is a visual indicator of cosine similarity (0.0â€“1.0 scale). Higher similarity = longer/darker bar. This helps users see how closely each document matched their query without needing to understand the number.

**Cross-category results**: Research mode searches across ALL categories by default. The category badge on each result (e.g., `executiveActions`, `civilService`, `igs`) shows which category scored the document. This is a feature, not a bug â€” a question about IG independence should surface documents from `igs`, `civilService`, and `executiveActions` if they're all relevant.

**Date range caveat**: Always displayed. Users must know what time period the answer covers. If the corpus only goes back to January 2025, an answer about "historical IG changes" would be misleading without this context.

**Source limitation caveat**: The synthesized answer is grounded exclusively in government documents. The News Coverage panel provides media context but is explicitly separated. If the system lacks court filings, congressional testimony, or other non-news external sources (Phase 11 alternative sources), a note appears: "Court filings and congressional records are not yet included in this search."

**Record vs. Coverage â€” the core design principle**: The government record and news coverage are two different lenses on the same events. A Federal Register document says "we are doing X." A news article says "the administration is doing X, and here's the context, the reaction, and what critics say." Showing them together gives the user something neither source provides alone. But they must be visually and structurally separated:

- The synthesized answer is built from government documents only â€” this is the system's unique value
- Government documents are labeled with ğŸ“„ and listed first under "GOVERNMENT RECORD"
- News articles are labeled with ğŸ“° and listed separately under "NEWS COVERAGE"
- The News Coverage panel explicitly states: "not used in the answer above"
- The LLM never sees news articles during synthesis â€” they cannot influence the answer

This separation preserves the system's core credibility claim ("we analyze the documentary record itself") while giving users the richer context they need.

**Related questions**: Generated by the LLM as part of the synthesis step. These are follow-up queries the user might want to explore. Clicking one submits it as a new search. Limit to 3.

**No answer is a valid answer**: If the retrieved documents don't contain enough information to answer the question, the system should say so explicitly: "The documentary record in our corpus does not contain enough information to answer this question. This may mean the topic is not reflected in Federal Register publications, or that relevant documents fall outside our current date range."

### 3.4 Research Mode â€” Detailed View Additions

In Detailed mode (reading level toggle), each source document in the results list expands to show:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [1] â–ˆâ–ˆ 0.94  Executive Order: Accountability for IGs      â”‚
â”‚      Jan 24, 2025 Â· executiveActions Â· Score: 14.2          â”‚
â”‚                                                             â”‚
â”‚      Matched keywords: IG removal (capture), inspector      â”‚
â”‚        general independence (drift), accountability (warn)  â”‚
â”‚      Suppressed: oversight (routine governance context)     â”‚
â”‚      Document class: executive_order Ã— 1.5                  â”‚
â”‚      AI reviewer: Confirmed Drift (confidence 0.88)         â”‚
â”‚                                                             â”‚
â”‚      "...removal of inspectors general who have failed      â”‚
â”‚      to meet performance standards established by the       â”‚
â”‚      administration..."                                     â”‚
â”‚      â†’ federalregister.gov/documents/2025/01/24/...         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This lets developers verify: "Did the system score this document correctly? Were the right keywords matched? Was the AI reviewer's assessment reasonable?"

---

## 4. Explore Mode â€” "Show Me the Data"

### 4.1 How It Works (Backend)

Explore mode combines traditional keyword/filter search with semantic similarity. The user can:

1. **Text search**: Full-text search against document titles and content (PostgreSQL `tsvector` or `ILIKE`)
2. **Semantic search**: If the query is more than 3 words, also run embedding similarity search and merge results
3. **Filter**: Category, date range, document class, score range, keyword tier, AI agreement/disagreement
4. **Sort**: By date, by score, by relevance (cosine similarity), by AI confidence

**API endpoint**: `GET /api/search?q={query}&mode=explore&category=...&source=...&dateFrom=...&dateTo=...&scoreMin=...&scoreMax=...&class=...&sort=...`

### 4.2 Explore Mode Results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search the Documentary Record                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ğŸ”  â”‚
â”‚  â”‚  schedule F reclassification                      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  Mode: [â—‹ Research | â— Explore]                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  FILTERS                                                    â”‚
â”‚  Category: [All â–¾]   Date: [2025-01-20] to [2026-02-12]    â”‚
â”‚  Score: [0] to [any]   Class: [All â–¾]                       â”‚
â”‚  Source: [All â–¾ | ğŸ“„ Government | ğŸ“° News]                  â”‚
â”‚  Show only: â–¡ AI disagreements  â–¡ Capture-tier matches      â”‚
â”‚             â–¡ Suppressed matches  â–¡ Unembedded docs         â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  243 results Â· sorted by [Relevance â–¾]                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Personnel Actions: Schedule F Implementation       â”‚    â”‚
â”‚  â”‚  Feb 10, 2025 Â· civilService Â· executive_order      â”‚    â”‚
â”‚  â”‚  ğŸ“„ Federal Register                                â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Score: 16.8  Â·  Matches: 3C Â· 4D Â· 2W             â”‚    â”‚
â”‚  â”‚  Suppressed: 1 (routine governance)                 â”‚    â”‚
â”‚  â”‚  AI: Confirmed Capture (0.91)                       â”‚    â”‚
â”‚  â”‚  Keywords: âœšschedule F âœšreclassification            â”‚    â”‚
â”‚  â”‚            âœšpolitical appointment  â—†merit system    â”‚    â”‚
â”‚  â”‚            â—†career protection  â—‹oversight            â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  "...reclassification of positions of a confidentialâ”‚    â”‚
â”‚  â”‚  policy-determining, policy-making, or policy-      â”‚    â”‚
â”‚  â”‚  advocating character..."                           â”‚    â”‚
â”‚  â”‚  â†’ federalregister.gov/documents/2025/02/10/...     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Civil Service Reform: Excepted Service Expansion   â”‚    â”‚
â”‚  â”‚  Mar 1, 2025 Â· civilService Â· final_rule            â”‚    â”‚
â”‚  â”‚  ğŸ“„ Federal Register                                â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Score: 12.4  Â·  Matches: 2C Â· 3D Â· 1W             â”‚    â”‚
â”‚  â”‚  Suppressed: 0                                      â”‚    â”‚
â”‚  â”‚  AI: Downgraded to Drift (0.74) âš                    â”‚    â”‚
â”‚  â”‚  Keywords: âœšschedule F  â—†excepted service           â”‚    â”‚
â”‚  â”‚            â—†merit system  â—‹civil service reform      â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  "...expansion of excepted service categories to    â”‚    â”‚
â”‚  â”‚  include positions previously under competitive..."  â”‚    â”‚
â”‚  â”‚  â†’ federalregister.gov/documents/2025/03/01/...     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  ... (241 more results)                                     â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Export: [CSV â†“]  [JSON â†“]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 Explore Mode Design Notes

**Keyword tier indicators**: `âœš` = capture (most severe), `â—†` = drift, `â—‹` = warning. Suppressed keywords shown with strikethrough in Detailed mode. These are compact enough to scan quickly but informative enough that a developer can immediately see the scoring breakdown.

**Source type filter**: The corpus includes both government documents (Federal Register, GAO, White House) and news articles (GDELT media coverage). The source filter lets users focus on one or the other, or see both interleaved. Each result card shows a source badge: ğŸ“„ for government documents, ğŸ“° for news articles. News articles go through the same keyword scoring pipeline as government documents â€” the source filter is purely about provenance, not methodology. For methodology investigators, comparing how the system scores a government announcement vs. news coverage of the same event is a useful diagnostic.

**AI disagreement filter**: The most valuable filter for methodology investigation. "Show me only documents where the AI reviewer disagreed with the keyword engine." This surfaces the exact edge cases where the methodology might need adjustment. Documents where the AI downgraded are marked with `âš `.

**"Suppressed matches" filter**: Shows documents where keywords matched but suppression rules fired. Useful for verifying that suppression rules aren't hiding genuine signals.

**"Unembedded docs" filter**: A developer/admin tool â€” shows documents that failed embedding (API error, content too long, etc.) so the corpus can be repaired.

**Score range filter**: Lets developers investigate threshold effects. "Show me everything in `courts` that scored between 5 and 8" â€” the ambiguous middle range where the difference between Stable and Drift is most uncertain.

**Export**: Both CSV and JSON. Includes all scoring metadata (matched keywords, suppressed keywords, AI assessment, document class, multiplier). Follows the export conventions from UI spec Â§12.

### 4.4 Explore Mode â€” Find Similar Documents

Each document result card has a "Find similar â†’" link. Clicking it runs a vector similarity search using that document's embedding as the query, returning the most semantically similar documents across the entire corpus.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents similar to:                                      â”‚
â”‚  "Personnel Actions: Schedule F Implementation"             â”‚
â”‚  (Feb 10, 2025 Â· civilService)                              â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  Same category (civilService):                              â”‚
â”‚  â–ˆâ–ˆ 0.93  Civil Service Reform: Excepted Service Expansion  â”‚
â”‚  â–ˆâ–ˆ 0.89  OPM Guidance: Position Reclassification Criteria  â”‚
â”‚  â–ˆâ–ˆ 0.84  Merit Systems Protection Board: Reduced Scope     â”‚
â”‚                                                             â”‚
â”‚  Other categories:                                          â”‚
â”‚  â–ˆâ–ˆ 0.81  Executive Order: Accountability for IGs           â”‚
â”‚           (igs)                                             â”‚
â”‚  â–ˆâ–ˆ 0.78  Agency Reorganization: HHS Workforce Reduction    â”‚
â”‚           (executiveActions)                                 â”‚
â”‚  â–ˆâ–ˆ 0.72  DOJ Personnel Reassignment Directive              â”‚
â”‚           (courts)                                          â”‚
â”‚                                                             â”‚
â”‚  [â† Back to search results]                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this matters**: Cross-category similarity reveals when related actions are happening across multiple institutional domains simultaneously. A Schedule F reclassification in `civilService` that's semantically similar to an IG removal order in `igs` and a DOJ reassignment in `courts` is a convergence signal â€” the kind of pattern the infrastructure convergence analysis is designed to catch, but surfaced here through direct exploration.

The results split into "Same category" and "Other categories" to make cross-category patterns immediately visible.

---

## 5. Shared Features (Both Modes)

### 5.1 P2025 Proximity Indicator

When a document has matches in the `p2025_matches` table (cosine similarity â‰¥ 0.75 to any P2025 proposal), show a small badge on the result card:

```
  â”‚  Score: 16.8  Â·  Matches: 3C Â· 4D Â· 2W                â”‚
  â”‚  P2025: Similar to "Reclassify federal employees" (0.89)â”‚
```

This links the document to the specific P2025 proposal it resembles. Clicking the P2025 badge navigates to the P2025 comparison page filtered to that proposal.

Available in both Research and Explore modes. In Research mode, the synthesized answer can note: "Several of the retrieved documents align with Project 2025 proposals on civil service reform [1, 2, 5]."

### 5.2 Rhetoric-Action Link

When a document matches keywords from a policy area tracked in the `intent_statements` table, and rhetoric on that topic preceded the document by 1â€“8 weeks, show:

```
  â”‚  Rhetoric trail: Administration discussed "Schedule F"   â”‚
  â”‚  in 3 briefings starting Jan 28, 2025 (13 days prior)   â”‚
```

This connects the documentary record to the rhetoricâ†’action pipeline. Available in both modes, Detailed view only.

### 5.3 Temporal Context Bar

Above the results in both modes, a small timeline visualization shows the density of results over time:

```
  Results over time:
  Jan â–â–‚â–…â–‡â–ˆâ–ˆâ–…â–ƒâ–‚â– Feb â–â–â–‚â–ƒâ–…â–ƒâ–‚â–â–â– Mar â–â–â–â–‚â–‚â–â–â–â–â– Apr
         â†‘ spike week of Feb 3
```

This immediately shows whether results are concentrated in a specific period (suggesting a policy event) or spread evenly (suggesting ongoing activity). The spike annotation is auto-generated when any week has 2Ã— the average result count.

### 5.4 Search History (localStorage)

Recent searches are stored locally (not server-side) and shown as suggestions when the search field is focused. Maximum 20 entries. Cleared via a "Clear history" link.

```
  Recent searches:
  Â· Has the administration reduced IG independence?
  Â· schedule F reclassification
  Â· FOIA denial rates 2025
  Â· documents similar to "Executive Order 14201"
```

### 5.5 Shareable URLs

Every search state is encoded in the URL:

- `/search?q=IG+independence&mode=research`
- `/search?q=schedule+F&mode=explore&category=civilService&dateFrom=2025-01-20&sort=score`
- `/search?q=IG+removals&mode=explore&source=news` (news articles only)

Journalists can share specific searches. Developers can link to specific filter combinations in bug reports or methodology discussions.

---

## 6. Mobile Behavior

- Search input is full-width
- Mode toggle is full-width below the input
- Filters in Explore mode collapse into a "Filters" button that opens a bottom sheet
- Document result cards stack vertically, full-width
- "Find similar" is available via a "â‹¯" overflow menu on each card
- Export buttons move to the overflow menu
- Temporal context bar is hidden on mobile (not enough horizontal space for meaningful visualization)

---

## 7. API Endpoints

| Endpoint                           | Method | Description                                        |
| ---------------------------------- | ------ | -------------------------------------------------- |
| `/api/search`                      | GET    | Unified search â€” `mode=research` or `mode=explore` |
| `/api/search/similar/[documentId]` | GET    | Find documents similar to a specific document      |
| `/api/search/embed`                | POST   | Embed a query string (internal, used by search)    |

### 7.1 Research Mode Response

```typescript
interface ResearchSearchResponse {
  answer: string; // LLM-synthesized answer with [N] citations
  // (grounded in government documents only)
  documents: Array<{
    id: number;
    citationIndex: number; // [1], [2], etc. as used in answer
    title: string;
    url: string;
    publishedAt: string;
    sourceType: string; // 'federal_register', 'gao', 'whitehouse', etc.
    category: string;
    documentClass: string;
    finalScore: number;
    cosineSimilarity: number;
    snippet: string; // ~200 char excerpt most relevant to query
    p2025Match?: {
      proposalId: string;
      proposalSummary: string;
      similarity: number;
    };
    rhetoricTrail?: {
      policyArea: string;
      statementCount: number;
      earliestStatement: string;
      lagDays: number;
    };
  }>;
  relatedNews: Array<{
    id: number;
    title: string;
    url: string;
    publishedAt: string;
    sourceType: string; // 'gdelt', 'news'
    sourceName: string; // 'Washington Post', 'ProPublica', etc.
    cosineSimilarity: number;
    snippet: string;
  }>;
  dateRange: { earliest: string; latest: string };
  queryConfidence: number; // avg cosine similarity of top results
  relatedQuestions: string[]; // max 3
}
```

### 7.2 Explore Mode Response

```typescript
interface ExploreSearchResponse {
  totalResults: number;
  page: number;
  pageSize: number; // default 20
  documents: Array<{
    id: number;
    title: string;
    url: string;
    publishedAt: string;
    sourceType: string; // 'federal_register', 'gao', 'gdelt', etc.
    sourceName?: string; // for news: 'Washington Post', etc.
    category: string;
    documentClass: string;

    // Scoring details
    severityScore: number;
    finalScore: number;
    classMultiplier: number;
    captureCount: number;
    driftCount: number;
    warningCount: number;
    suppressedCount: number;
    matchedKeywords: Array<{
      keyword: string;
      tier: 'capture' | 'drift' | 'warning';
    }>;
    suppressedKeywords: Array<{
      keyword: string;
      rule: string;
      reason: string;
    }>;

    // AI assessment
    aiRecommendedStatus: string;
    aiConfidence: number;
    aiAgreesWithKeywords: boolean;

    // Similarity (if semantic search was used)
    cosineSimilarity?: number;

    // Cross-references
    p2025Match?: { proposalId: string; similarity: number };
    rhetoricTrail?: { policyArea: string; lagDays: number };

    snippet: string;
  }>;
}
```

---

## 8. Implementation Notes

### 8.1 pgvector Query Performance

The `documents` table with 1536-dimensional embeddings needs an IVFFlat or HNSW index for acceptable query performance at scale:

```sql
-- HNSW index (preferred â€” better recall, slightly more memory)
CREATE INDEX idx_documents_embedding_hnsw
  ON documents USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
```

At the current corpus size (thousands of documents), brute-force cosine similarity may be fast enough. The HNSW index becomes necessary above ~50K documents. Add it proactively â€” the index build is a one-time cost.

### 8.2 Rate Limiting

Research mode involves an LLM call per query, which has cost implications:

- Research mode: 5 queries per minute per IP, 50 per hour
- Explore mode: 20 queries per minute per IP (no LLM cost)
- Similar document search: 10 per minute per IP

Display a clear message when rate-limited: "To keep this service free, we limit searches to N per hour. Your limit resets at HH:MM."

### 8.3 Cost Management

Research mode LLM synthesis is the primary cost driver. Mitigation strategies:

1. **Cache common queries**: Hash the query + date range, cache the synthesized answer for 24 hours. Many users will ask similar questions.
2. **Limit context window**: Send top 8 documents to the LLM, not all 20 retrieved. The remaining 12 are shown in the source list but not synthesized over.
3. **Use Haiku for synthesis**: The synthesis task (summarize these documents to answer a question) doesn't require Opus-level reasoning. Sonnet or Haiku with good grounding prompts will produce comparable results at lower cost.
4. **Degrade gracefully**: If the LLM budget is exhausted, fall back to Explore mode with a message: "AI-assisted answers are temporarily unavailable. Showing document results only."

### 8.4 Embedding Compatibility

The search query must be embedded with the same model used for document embeddings. If the model changes (e.g., migrating from `text-embedding-3-small` to a future model), all document embeddings must be recomputed. Store the embedding model identifier in a config constant and validate it matches at query time.

### 8.5 Content Snippets

Snippets for search results should be generated at query time, not stored. For semantic search, extract the ~200 character window of the document content that has the highest token overlap with the query. For keyword search, highlight the matched keyword in context. If the document has no `content` field (only title + metadata), use the title as the snippet.

---

## 9. Backend Dependencies

| Feature                  | Requires                                     | Status                                           |
| ------------------------ | -------------------------------------------- | ------------------------------------------------ |
| Semantic search          | `documents.embedding` column populated       | Exists (migration 0005)                          |
| Document scoring details | `document_scores` table                      | Exists (migration 0007)                          |
| P2025 proximity          | `p2025_matches` table                        | Exists (migration 0010)                          |
| Rhetoric trail           | `intent_statements` + `intent_weekly` tables | Exists (migrations 0002, 0009)                   |
| Baseline centroids       | `baselines.embedding_centroid` column        | Exists (migration 0008)                          |
| Full-text search index   | `tsvector` on documents.title + content      | **New â€” needs migration**                        |
| HNSW vector index        | Index on documents.embedding                 | **New â€” needs migration**                        |
| Search query cache       | Cache table or Redis                         | **New** (can use existing `cache_entries` table) |

### 9.1 New Migration: Full-Text Search + Vector Index

```sql
-- Full-text search support
ALTER TABLE documents ADD COLUMN IF NOT EXISTS search_vector tsvector
  GENERATED ALWAYS AS (
    setweight(to_tsvector('english', coalesce(title, '')), 'A') ||
    setweight(to_tsvector('english', coalesce(content, '')), 'B')
  ) STORED;

CREATE INDEX idx_documents_search_vector ON documents USING gin (search_vector);

-- HNSW vector index for semantic search
CREATE INDEX idx_documents_embedding_hnsw
  ON documents USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
```

---

## 10. Sprint Estimate

This feature spans two sprints, targeting 250â€“350 lines each:

### Sprint L (Search Infrastructure + Explore Mode)

1. Full-text search migration (tsvector + HNSW index)
2. Search service â€” `lib/services/search-service.ts` (embedding, similarity search, keyword search, filter/sort)
3. Explore mode API endpoint â€” `/api/search?mode=explore`
4. "Find similar" endpoint â€” `/api/search/similar/[documentId]`
5. Search page UI â€” input, mode toggle, Explore mode results with filters

**Deliverable**: Developers can search the document corpus with full scoring details, filter by category/date/score/class, find similar documents across categories, and export results.

### Sprint M (Research Mode + Cross-References)

1. RAG synthesis service â€” `lib/services/research-service.ts` (query embedding, retrieval, re-ranking, LLM grounding prompt, answer generation)
2. Research mode API endpoint â€” `/api/search?mode=research`
3. Research mode UI â€” synthesized answer with citations, source document list, related questions
4. P2025 proximity badges and rhetoric trail indicators (both modes)
5. Temporal context bar
6. Query caching (use existing `cache_entries` table)
7. Rate limiting middleware

**Deliverable**: Journalists and academics can ask natural language questions and get grounded answers citing specific government documents. Cross-references to P2025 proposals and rhetoric trail are visible on all search results.

---

## 11. What This Does NOT Include

- **Real-time document ingestion**: Search operates on the existing weekly-ingested corpus. Documents are not added to the search index in real-time.
- **User accounts or saved searches**: No server-side search history. Local storage only.
- **Collaborative annotations**: No ability to tag, bookmark, or comment on documents within the search interface. This could be a future feature for academic research teams.
- **News in synthesized answers**: News articles from the GDELT corpus appear in the News Coverage panel but are never used in the LLM-synthesized answer. The answer is grounded exclusively in government documents. This is a deliberate design choice, not a limitation.
- **Court filings and congressional record**: These are not in the corpus unless Phase 11 alternative sources have been implemented. The source limitation caveat makes this explicit when relevant.
- **Conversational follow-ups**: Each Research mode query is independent. There is no multi-turn conversation with the LLM. The "related questions" feature provides a lightweight alternative.
- **News source filtering by outlet**: The News Coverage panel shows all matching news articles ranked by relevance. There is no filter to show only articles from specific outlets (e.g., "only AP and Reuters"). This could be added if users request it, but the initial implementation keeps it simple.
